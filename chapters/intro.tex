\section{Introduction}

Image processing is everywhere: take programs like Photoshop or Photopea for example. But for real-time applications like video processing or shaders, this becomes a lot more difficult.

If a CPU has to do all this sequentially, it would struggle to get real-time performance. Take this simple example: we have HD video (1920 pixels by 1080 pixels, 60 fps) we want to process. We would have to process $1920 \cdot 1080 \cdot 60 = 124416000$ pixels per second, giving us only 8,04 ns per pixel. Doing this sequentially on a CPU, means that the CPU has a very high workload, or might even be impossible.

Instead, you can offload this work to a co-processor, that will process an image in parallel. This makes the whole system faster, and the CPU has time to do other work.


In image processing, a kernel is a small matrix that can be used to add an effect to an image, by convoluting the kernel over the image. Different kernels achieve different tasks, like blurring, sharpening, embossing\dots  The output of a pixel $A$ does not have any effect on a different pixel $B$, meaning we can fully parallelize the process.



\section{Literature study}

To implement the design, we first looked into how generic GPU programming is done. In \cite{nvidia_cuda_image_processing_2008}, a kernel image processor is implemented in CUDA. In this example, the image is split into blocks and given to a number of GPU cores. Each multiprocessor has access to a 16 kB ultra-fast shared memory, allowing it to store $64 \times 64$ pixels, with 4 bytes per pixel. Those $64 \times 64$, after its loaded in from global memory, can then be processed in parallel.

In this paper, a hardware version of the algorithm described in \cite{nvidia_cuda_image_processing_2008} is implemented.
