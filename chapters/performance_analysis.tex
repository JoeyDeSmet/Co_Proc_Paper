\section{Performance Analysis}

In this section, we evaluate the performance of the proposed convolution co-processor. Metrics include processing throughput, latency, resource utilization, and energy efficiency. Comparisons are made with a reference CPU-only implementation on the ZYNQ7000 processing system.

\subsection{Experimental Setup}
The experiments were performed on a Digilent ZedBoard development board with the following specifications:

\begin{itemize}[noitemsep]
  \item Processing System: Dual-core ARM Cortex-A9, 667 MHz
  \item FPGA: XC7Z020 (Artix-7), 53k LUTs, 106k FFs, 4.9 Mb BRAM
  \item Clock frequency of co-processor: \SI{100}{\MHz}
  \item Test images: resolution $640 \times 480$, 32-bit RGBA
  \item Convolution kernel: $3\times3$
\end{itemize}

\subsection{Latency and Throughput}

For this performance analysis, we'll compare the co-processor speed to a naïve sequential algorithm on a CPU. This algorithm is shown in \Cref{lst:convolution_sequential}.

\begin{lstlisting}[language=Python,label=lst:convolution_sequential]
  for y from 1 to H-2:
    for x from 1 to W-2:
      acc = 0
      # Convolute current pixel with kernel
      for ky from -1 to +1:
        for kx from -1 to +1:
          acc += img[y + ky][x + kx] \
            * K[ky + 1][kx + 1]
      out[y][x] = acc
\end{lstlisting}

In the naïve implementation, we need 9 multiplications and 9 additions per pixel. Taking one cycle per addition, one cycle per multiplication, and 10 to 15 cycles for the loop logic and memory, we need about 28 to 33 cycles/pixel in the sequential example.

In our parallel implementation, we have an initial three line buffers that need to be filled first (taking 640 cycles each), and an additional three cycles to load the needed data into the convolution unit. After this first delay, we can process pixels at a rate of 1 cycle/pixel.

Processing the image by splitting it into four quadrants, we get an additional speedup of 4x. % Or 400\%? But just saying "400\%" is ambiguous: is it x*400% or x+400%*x?

When we look at our co-processor architectural implementation, we only need 1 cycle/pixel after the initial latency for filling the buffers and the data going trough the pipeline. So our implementation already has an architectural speedup of 28-33X, when also taking into account that we split the image in four we can multiply this speedup buy four and have a total architectural speedup of 112-132X.

However, the speedup is limited by the speed of AXI.

\begin{equation}
  T_{AXI} = 2 \cdot \frac{N_{pixels}}{\mathrm{pixels\_per\_AXI\_cycle} \cdot f_{AXI}}
\end{equation}

The latency $T_\text{latency}$ of the co-processor is measured as the time between issuing a convolution request and receiving the processed data:
\begin{align}
  T_\text{latency} &= T_\text{transfer} + T_\text{compute} + T_\text{response} \\
    &= (640 \cdot 3+3) + (640) + (640) \\
    &= 3203~\text{clock cycles}
\end{align}

Throughput $R_\text{throughput}$ is calculated as:
\begin{equation}
R_\text{throughput} = \frac{\text{Number of pixels processed}}{T_\text{latency}}
\end{equation}

\begin{table}[H]
\centering
\caption{Latency and throughput for processing new versus in-memory images}
\begin{tabular}{c c c}
\hline
In memory & Latency [ms] & Throughput [MPix/s] \\
\hline
No & -- & -- \\
Yes & -- & -- \\
\hline
\end{tabular}
\label{tab:latency-throughput}
\end{table}

\subsection{Resource Utilization}

% The FPGA resource usage of the convolution co-processor is summarized in Table~\ref{tab:resource-util}:

For a single convolution unit (which consists of three line buffers of 640 pixels depth, and a processor unit), the estimated resource utilization of the synthesized design is summarized in \Cref{tab:resource-util}.

\begin{table}[H]
\centering
\caption{FPGA Resource Utilization}
\begin{tabular}{l c c}
\hline
% Resource & Used & Available \\
Resource & Used (\%) \\
\hline
% LUTs & -- & -- \\
% Flip-Flops & -- & -- \\
% BRAM [Kb] & -- & -- \\
% DSP Slices & -- & -- \\
LUTs & 4 \\
Flip-Flops & 1 \\
% BRAM [Kb] & -- \\
% DSP Slices & -- \\
BUFG & 3 \\
LUTRAM & 8 \\
\hline
\end{tabular}
\label{tab:resource-util}
\end{table}

Additionally, the convolution unit has a worst negative slack of 4,363 ns, well within the constraints for a 100 MHz FPGA clock speed.

\subsection{Comparison with CPU Implementation}

For reference, a CPU-only implementation, as a FreeRTOS task with highest priority, was run on the ARM Cortex-A9 core. Table~\ref{tab:cpu-vs-fpga} summarizes the speed-up achieved:

\begin{table}[H]
\centering
\caption{Speed-Up of FPGA Co-Processor vs CPU}
\begin{tabular}{c c}
\hline
CPU Latency [ms] & FPGA Speed-Up \\
\hline
-- & -- \\
\hline
\end{tabular}
\label{tab:cpu-vs-fpga}
\end{table}

% If possible and if we have time enough
\subsection{Energy Efficiency}

Energy consumption was measured for the convolution co-processor using onboard power monitoring or external measurement tools. The energy efficiency $\eta$ is defined as the number of pixels processed per joule of energy consumed:

\begin{equation}
\eta = \frac{\text{Number of pixels processed}}{E_\text{total}} \quad [\text{MPixels/J}]
\end{equation}

where $E_\text{total}$ is the total energy consumed during the convolution operation.

\begin{table}[H]
\centering
\caption{Energy efficiency of the co-processor for CPU and FPGA}
\begin{tabular}{c c c}
\hline
Platform & Energy [mJ] & Efficiency [MPix/J] \\
\hline
$FPGA$ & -- & -- \\
$CPU$ & -- & -- \\
\hline
\end{tabular}
\label{tab:energy-efficiency}
\end{table}