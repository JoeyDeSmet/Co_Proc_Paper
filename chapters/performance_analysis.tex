\section{Performance Analysis}

In this section, we evaluate the performance of the proposed convolution co-processor. Metrics include processing throughput, latency, resource utilization, and energy efficiency. Comparisons are made with a reference CPU-only implementation on the ZYNQ7000 processing system.

\subsection{Experimental Setup}
The experiments were performed on a Digilent ZedBoard development board with the following specifications:

\begin{itemize}[noitemsep]
  \item Processing System: Dual-core ARM Cortex-A9, 667 MHz
  \item FPGA: XC7Z020 (Artix-7), 53k LUTs, 106k FFs, 4.9 Mb BRAM
  \item Clock frequency of co-processor: \SI{100}{\MHz}
  \item Test images: resolution $640 \times 480$, 32-bit RGBA
  \item Convolution kernel: $3\times3$
\end{itemize}

\subsection{Latency and Throughput}

For this performance analysis, we'll compare the co-processor speed to a naïve sequential algorithm on a CPU. This algorithm is shown in \Cref{fig:convolution_sequential}.
\begin{figure}[H]
\begin{lstlisting}[language=Python]
for y from 1 to H-2:
  for x from 1 to W-2:
    acc = 0
    # Convolute current pixel with kernel
    for ky from -1 to +1:
      for kx from -1 to +1:
        acc += img[y + ky][x + kx] \
          * K[ky + 1][kx + 1]
    out[y][x] = acc
\end{lstlisting}
\caption{Implementation of the sequential convolution algorithm}
\label{fig:convolution_sequential}
\end{figure}
In the naïve implementation, we need 9 multiplications and 9 additions per pixel. Taking one cycle per addition, one cycle per multiplication, and 10 to 15 cycles for the loop logic and memory, we need about 28 to 33 cycles/pixel in the sequential example.

In our parallel implementation, we have an initial three line buffers that need to be filled first (taking 640 cycles each), and an additional three cycles to load the needed data into the convolution unit. After this first delay, we can process pixels at a rate of 1 cycle/pixel.

Processing the image by splitting it into four quadrants, we get an additional speedup of 4x. % Or 400\%? But just saying "400\%" is ambiguous: is it x*400% or x+400%*x?

When we look at our co-processor architectural implementation, we only need 1 cycle/pixel after the initial latency for filling the buffers and the data going trough the pipeline. So our implementation already has an architectural speedup of 28-33X, when also taking into account that we split the image in four we can multiply this speedup buy four and have a total architectural speedup of 112-132X.

This speedup would theoretically scale linearly with the clock speed, as shown in \Cref{fig:architectural_speedup}.

\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth]{images/architectural speedup vs PL frequency.png}
  \caption{Theoretical architectural speedup}
  \label{fig:architectural_speedup}
\end{figure}

However, the speedup is limited by the speed of AXI.
\begin{equation}
  T_{AXI} = 2 \cdot \frac{N_{pixels}}{\mathrm{pixels\_per\_AXI\_cycle} \cdot f_{AXI}}
  \label{eq:AXI_limited}
\end{equation}

In \Cref{fig:AXI_limited_speed_graph}, it's possible to see that AXI limits the system to around 2.2$\times$ speedup, even with very high clock speeds.

\begin{figure}
  \centering
  \includegraphics[width=0.45\textwidth]{images/AXI limited speedup.png}
  \caption{Plot of \Cref{eq:AXI_limited}; speedup is limited by AXI}
  \label{fig:AXI_limited_speed_graph}
\end{figure}

If this were a real GPU-like system, we would expect a needed data rate of $1920 \cdot 1080~\mathrm{pixels/frame} \cdot 32~\mathrm{bits/pixel} \cdot 60~\mathrm{frames/second} = 497.7~\mathrm{MB/s}$ (!).

The latency $T_\text{latency}$ of the co-processor is measured as the time between issuing a convolution request and receiving the processed data:
\begin{align*}
  T_\text{latency} &= 640 \cdot 3+3 \\
    &= 1923~\text{clock cycles}
\end{align*}

% Throughput $R_\text{throughput}$ is calculated as:
% \begin{equation}
% R_\text{throughput} = \frac{\text{Number of pixels processed}}{T_\text{latency}}
% \end{equation}

% TODO: what's this supposed to be?
% \begin{table}[H]
% \centering
% \caption{Latency and throughput for processing new versus in-memory images}
% \begin{tabular}{c c c}
% \hline
% In memory & Latency [ms] & Throughput [MPix/s] \\
% \hline
% No & -- & -- \\
% Yes & -- & -- \\
% \hline
% \end{tabular}
% \label{tab:latency-throughput}
% \end{table}

\subsection{Resource Utilization}

% The FPGA resource usage of the convolution co-processor is summarized in Table~\ref{tab:resource-util}:

For a single convolution unit (which consists of three line buffers of 640 pixels depth, and a processor unit), the estimated resource utilization of the synthesized design is summarized in \Cref{tab:resource-util}.

\begin{table}[H]
\centering
\caption{FPGA Resource Utilization}
\begin{tabular}{l c c}
\hline
% Resource & Used & Available \\
Resource & Used (\%) \\
\hline
% LUTs & -- & -- \\
% Flip-Flops & -- & -- \\
% BRAM [Kb] & -- & -- \\
% DSP Slices & -- & -- \\
LUTs & 4 \\
Flip-Flops & 1 \\
% BRAM [Kb] & -- \\
% DSP Slices & -- \\
BUFG & 3 \\
LUTRAM & 8 \\
\hline
\end{tabular}
\label{tab:resource-util}
\end{table}

Additionally, the convolution unit has a worst negative slack of 4,363 ns, well within the constraints for a 100 MHz FPGA clock speed.

\subsection{Comparison with CPU Implementation}

For reference, we would've liked to run a CPU-only implementation, but due to time constraints this hasn't happened. Instead, this was done theoretically (see \Cref{fig:convolution_sequential}). There, we assumed 10 to 15 cycles needed for memory access and the loop logic. A summary of these findings is shown in \Cref{tab:cpu-vs-fpga}.

\begin{table}[H]
\centering
\caption{Speed-Up of FPGA Co-Processor vs CPU}
\begin{tabular}{lll}
\hline
     & Latency (cycles) & Throughput (max, cycles/pixel) \\ \hline 
CPU  & 15               & 33                             \\
FPGA & 1923             & 1                              \\ \hline
\end{tabular}
\label{tab:cpu-vs-fpga}
\end{table}

For the sequential algorithm, processing a 640 $\times$ 480 pixel image, we would expect it to take $15 + 33 \cdot 640 \cdot 480 = 10137615$ cycles. Compared to the FPGA, where the image is split into four blocks that's processed in parallel, meaning we would expect $1923 + 1 \cdot 640 \cdot 480 \cdot \frac{1}{4} = 78723$ cycles, which is still significantly faster ($128.8\times$). Off course, the clock speed of these implementations will be different, resulting in a slightly different outcome.

The clock speed of the CPU is set at 667 MHz, while the FPGA is at 100 MHz. Still, the theoretical speedup of the parallel algorithm significantly outweighs this difference.

% TODO: remove this? just don't care about energy efficiency?
% % If possible and if we have time enough
% \subsection{Energy Efficiency}

% Energy consumption was measured for the convolution co-processor using onboard power monitoring or external measurement tools. The energy efficiency $\eta$ is defined as the number of pixels processed per joule of energy consumed:

% \begin{equation}
% \eta = \frac{\text{Number of pixels processed}}{E_\text{total}} \quad [\text{MPixels/J}]
% \end{equation}

% where $E_\text{total}$ is the total energy consumed during the convolution operation.

% \begin{table}[H]
% \centering
% \caption{Energy efficiency of the co-processor for CPU and FPGA}
% \begin{tabular}{c c c}
% \hline
% Platform & Energy [mJ] & Efficiency [MPix/J] \\
% \hline
% $FPGA$ & -- & -- \\
% $CPU$ & -- & -- \\
% \hline
% \end{tabular}
% \label{tab:energy-efficiency}
% \end{table}